{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的原理较为简单，表示起来和数据结构中的图没有什么区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树的构造"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树的构造需要解决的第一个问题就是：如何选择划分的依据呢？即，如何选择划分数据分类的特征？  \n",
    "《机器学习实战》中介绍了ID3算法--迭代二叉树3代。这个算法的核心思想是利用**信息增益**来度量属性的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 信息增益"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先需要知道两个定义。如果待分类的事务可能划分在多个类中（即该事务不属于同一个分类，这是决策树停止划分的条件），则：\n",
    "1. xi的信息定义为**L(xi)=-log2p(xi)** 。其中，p(xi)为选择该分类的概率。\n",
    "2. 熵 **H = -Σ(i=1~n)p(xi)log2p(xi)** 。   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据这个公式写出计算香农熵的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算熵\n",
    "from math import log\n",
    "def calShannonEnt(dataset):\n",
    "    numEntries = len(dataset)  #数据总数\n",
    "    labelCounts = {}  #计算各个分类的总量\n",
    "    for i in dataset:\n",
    "        currLabel = i[-1]  #获取当前标签\n",
    "        if currLabel not in labelCounts:\n",
    "            labelCounts[currLabel] = 0  \n",
    "        labelCounts[currLabel]+=1   #统计标签出现次数\n",
    "    shannonEnt = 0.0\n",
    "    for i in labelCounts:\n",
    "        prob = float(labelCounts[i])/numEntries\n",
    "        shannonEnt -= prob * log(prob,2)  #按照公式计算\n",
    "    return shannonEnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9709505944546686"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#测试calShannonEnt函数\n",
    "data = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[0,1,'no']]\n",
    "calShannonEnt(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵反映了一个数据集的无序程度。换句话说，熵越高，则混合的数据就越多。（浅显的理解就是分类越多熵越高）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 划分数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 首先，先完成一个按照给定特征划分数据集的程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 'no'], [1, 'no']]\n",
      "[[1, 'yes'], [1, 'yes'], [0, 'no']]\n"
     ]
    }
   ],
   "source": [
    "# 按给定特征划分数据集\n",
    "def splitDataSet(dataset,axis,value):  #dataset是数据集，axis是给定的特征，value是期望的该特征的值\n",
    "    newDataSet = []\n",
    "    for data in dataset:\n",
    "        if data[axis] == value:  #如果该条数据的axis值等于value\n",
    "            reducedData = data[:axis]  #去掉该特征\n",
    "            reducedData.extend(data[axis+1:])  #以上两步实际上相当于pandas的drop\n",
    "            newDataSet.append(reducedData)\n",
    "    return newDataSet\n",
    "\n",
    "#将上述data数据按照第0维特征划分\n",
    "print(splitDataSet(data,0,0))  #第0维特征等于0\n",
    "print(splitDataSet(data,0,1))  #第0维特征等于1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yes', 'yes', 'no', 'no', 'no']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subdataset = [example[2] for example in data]\n",
    "subdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 选择最佳的属性以划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选择最佳的属性以划分数据集\n",
    "def chooseBestFeatureToSplit(dataset):\n",
    "    numFeatures = len(dataset[0]) - 1  #计算特征维度\n",
    "    baseEntropy = calShannonEnt(dataset)\n",
    "    bestInfoGain = 0.0\n",
    "    bestFeatrue = -1\n",
    "    for i in range(numFeatures):\n",
    "        featureList = [example[i] for example in dataset]  #对于每一个特征（i），取出其所有可能的取值\n",
    "        featureSet = set(featureList)  #唯一化特征的取值\n",
    "        newEntropy = 0.0\n",
    "        for value in featureSet:\n",
    "            splitdataset = splitDataSet(dataset,i,value)  #对于第i个特征，将其按照所有的取值value划分数据集\n",
    "            prob = len(splitdataset)/float(len(dataset))  #计算取value时所占总数据集比例\n",
    "            newEntropy += calShannonEnt(splitdataset) * prob \n",
    "            #计算新的香农熵，划分后的信息熵需要乘上每种划分的概率\n",
    "            infoGain = baseEntropy - newEntropy  #计算信息受益\n",
    "        if infoGain > bestInfoGain:\n",
    "            bestInfoGain = infoGain\n",
    "            bestFeature = i  #找到获得最高信息受益的特征\n",
    "    return bestFeature\n",
    "\n",
    "chooseBestFeatureToSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 递归创建树"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majorityCnt(classList):\n",
    "    classCount={}\n",
    "    for vote in classList:\n",
    "        if vote not in classCount.keys():\n",
    "            classCount[vote]=0\n",
    "        classCount[vote] +=1\n",
    "    sortedClassCount = sorted(classCount.items(),key=operator.itemgetter(1),reverse=True)\n",
    "    return sortedClassCount[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataset,labels):\n",
    "    classlist = [example[-1] for example in dataset]   #获取当前数据集的所有类别\n",
    "    if classlist.count(classlist[0]) == len(classlist):   #如果列表中第一个值(classlist[0])的个数等于列表长度，换言之，所有数据的分类相同\n",
    "        return classlist[0]\n",
    "    if len(dataset[0]) == 1:  #数据集的特征只剩下一个，换言之，所有特征都已使用完，并且还不能使数据集完全分类(因为没有触发第一个停止条件)\n",
    "        return majorityCnt(classList)\n",
    "    bestFeat = chooseBestFeatureToSplit(dataset)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "    myTree = {bestFeatLabel:{}}\n",
    "    del(labels[bestFeat])\n",
    "    #这里开始就和之前的思路一样了，找出选择的最好特征值的所有取值，按每一个取值划分数据集。对于每一个划分后的子数据集，都重新构建一个子树。\n",
    "    featLabels = [example[bestFeat] for example in dataset]\n",
    "    uniqueVals = set(featLabels)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]\n",
    "        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataset,bestFeat,value),subLabels)\n",
    "    return myTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}\n"
     ]
    }
   ],
   "source": [
    "labels = ['no surfacing','flippers']\n",
    "myTree = createTree(data,labels)\n",
    "print(myTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
